\subsection{Co-Saliency based Color Mapping}
\label{subsec:solver}
On the basis of the co-saliency model, we meet DR1 and DR2 in two ways: co-saliency based color assignment and co-saliency based palette generation.

\vspace{1.5mm}
\noindent\textbf{Co-saliency based Color Assignment}.
Given a good color palette with $P$ colors ($P\geq M$), the optimal color mapping can be obtained by
taking the co-saliency model in Eq.~\ref{eq:cosaliency} as the objective of the state-of-the-art color assignment method~\cite{Wang2018}. Starting from a random permutation of $P$ colors, we use the simulated annealing algorithm~\cite{aarts1989stochastic} to find the optimal permutation with two randomized strategies to improve the solution. One is randomly exchanging two colors from the selected $m$ colors and the other is replacing one color from the $m$ selected colors with the one chosen from the unselected $P-M$ colors. With a few iterations, we can obtain a reasonable color mapping as shown in Fig.%~\ref{fig:teaser} (c).

However, this method has two major limitations: i) requiring users to try many palettes for selecting a good one; and ii) the design of most existing palettes is not oriented towards visual comparison so that even the best color assignment cannot provide prominent cues for this task.
%For example, all colors in the Tableau palette are highly discriminable and it is hardly to find a satisfactory solution, see Fig.~\ref{fig:teaser} (b). Thus, we prompt users to use our co-saliency based palette generation method.
For example, all colors in the ColorBrewer 8-class Set1~\cite{harrower2003colorbrewer} palette are highly discriminable, but it is hard to find a satisfactory solution. Fig.~\ref{fig:colorbrewer} shows an example, where the change of the red class is hard to identify at once even it is very distinctive. Thus, we prompt users to use our co-saliency based palette generation method.

\begin{figure}[!tb]
\centering
\includegraphics[width=.9\columnwidth]{figures/colorbrewer.pdf}
\caption{Visualizing the same data sets as shown in Fig.~\ref{} with the ColorBrewer palette and our assignment method.}
\vspace*{-3mm}
\label{fig:colorbrewer}
\end{figure}

\vspace{1.5mm}
\noindent\textbf{Co-saliency based Palette Generation}.
The recently proposed data-aware palette generation method~\cite{Lu21} automatically generates discriminable and preferable palettes by maximizing the combination of three palette quality measures: point distinctness, name difference, and color discrimination.
By replacing the first measure with our co-saliency model, the palette generation is formulated as an optimization problem:
\begin{equation}
\arg\max_{\mathbf{P}} E(\mathbf{P}) = \omega_0 E_{CoS} + \omega_1 E_{ND} + \omega_2 E_{CD}.
\label{eq:energyfunc}
\end{equation}
which consists of a co-saliency term $E_{CoS}$ (see Eq.~\ref{eq:cosaliency}), a name difference term $E_{ND}$ and a color discrimination term $E_{CD}$, balanced by $\omega_0$, $\omega_1$ and $\omega_2$. For more detail about $E_{ND}$ and $E_{CD}$, we refer readers to~\cite{Lu21}. By using the same optimization method as Lu et al.~\cite{Lu21}, we can generate desired colors in real time. %see Fig.~\ref{fig:teaser} (d).



\subsection{Parameter Effect}
\label{subsec:parameter}
Besides different weights for different terms in palette generation~\cite{Lu21}, our co-saliency model involves four parameters: the weight $\lambda$ between two contrasts, the threshold for the class importance $\kappa$, and $\nu$ that is related to the definition of the class change degree which is used as our default class importance.
Since $\nu$ is fixed in our experiments and the class importance can be specified by user, we mainly discuss the effects of $\lambda$  and $\kappa$.

\begin{figure}[!t]
\centering
\includegraphics[width=0.49\textwidth]{figures/lambda.pdf}
\caption{Effect of  $\lambda$: a small $\lambda$ results in high class contrast (a) while a large $\lambda$ highlights all changed classes (c). A proper value balances between these two factors (b).}
\vspace*{-3mm}
\label{fig:lambda}
\end{figure}
\vspace{.5em}

\noindent\textbf{Balancing Weight $\lambda$}.
Although this parameter modulates the influence between the local class contrast and the global background contrast, it offers a compromise between DR1 and DR2.
As shown in Fig.~\ref{fig:lambda}, a small $\lambda$ results in well-separated classes, but the contrast between unchanged and changed classes is small (see Fig.~\ref{fig:lambda}(a)); while a large $\lambda$ highlights the changed classes by lowering the color saturation of unchanged classes. This, however, might result in less class separability between the unchanged classes,  see the light blue class in Fig.~\ref{fig:lambda}(c).
This is reasonable, because  pre-attentive vision
% processing mechanism
lets a bright saturated color region within  regions of de-saturated colors ``pop-out'' to the viewer~\cite{healey1995visualizing}.
In our experiments, we found that setting  $\lambda=0.8$ as the default allows to simultaneously emphasize changes and preserve the discriminability between classes, see an example in Fig.~\ref{fig:lambda}(b).


\vspace{1.5mm}
\noindent\textbf{Change Threshold $\kappa$}.
The threshold $\kappa$ selects the classes with large change degrees to be highlighted.
With a default value of zero, all changed classes are ensured to be highlighted. Likewise, a large $\kappa$ will de-emphasize classes with a small change degree.
We further allow users to highlight classes with small changes to clearly show  subtle differences by using carefully designed thresholding functions (see Sec.~\ref{sec:extesnion}).


We can observe that when there's only one scatterplot and $\theta_i$ of each class is zero, then Equation.~\ref{eq:cosaliency} is very similar to the objective function of ~\cite{Wang2018}. Our method extends Wang et.al's work to multiple scatterplots. Besides, we add $\frac{1}{n^j_i}$ to emphasize the class with less points. As shown in Fig.~\ref{fig:nij}(b), with this new term, the little classes, like red, blue and purple class, become more discriminable.

\begin{figure}[!t]
\centering
\includegraphics[width=0.49\textwidth]{figures/nij.pdf}
\caption{Effect of $\frac{1}{n^j_i}$. (a)Without this term the small classes are hard to catch user's attention; (b)with this term, small classes are easy to find. Palettes are generated with same scatterplot.}
\vspace*{-3mm}
\label{fig:nij}
\end{figure}
\vspace{.5em}